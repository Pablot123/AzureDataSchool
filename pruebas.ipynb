{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\pablo.tamayo\\\\Desktop\\\\DataSchool\\\\Azure\\\\AzureDataSchool\\\\train_data.csv']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '20d4fdf3-6a4b-4f0b-a842-bd7392136332'\n",
    "resource_group = 'cienciadatos'\n",
    "workspace_name = 'azureml'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_id(workspace, '9227d1d4-d528-48db-84ed-768975adbf63')\n",
    "dataset.download(target_path='.', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%%writefile experiments/mlflow_preprocess.py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "impoer mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../airlines_delay/airlines_delay.csv\", sep = \",\")\n",
    "\n",
    "def clean_data(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Delete the Fligh column\n",
    "    input: dataset\n",
    "    output: pandas dataframe withiout Flight\n",
    "    '''\n",
    "    clean_df = data.drop('Flight', axis=1, inplace=False)\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def preprocess_data(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Preprocess the data converting the categorical columns to one hot encodig\n",
    "    and apply standarScaler to numeric columns\n",
    "    input: pandas dataFrame\n",
    "    output: tuple, first element corrempond to pd.Dataframe of independent features\n",
    "            second element is the dependent(target) feature \n",
    "    '''\n",
    "\n",
    "    numeric_columns = ['Length','Time']\n",
    "    category_columns = ['Airline', 'AirportFrom', 'AirportTo', 'DayOfWeek']\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                            ('numeric', numeric_transformer, numeric_columns),\n",
    "                            ('cat', categorical_transformer, category_columns)\n",
    "                        ],\n",
    "                        remainder='passthrough'\n",
    "                    )\n",
    "    \n",
    "    #split the dependent(y) and independent(df_x) features \n",
    "    df_X = data.drop('Class', axis=1, inplace=False)\n",
    "    #y_labels = data['Class']\n",
    "    y = data['Class']\n",
    "    preprocessed_data = preprocessor.fit_transform(df_X)\n",
    "    encoded_category = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(category_columns)\n",
    "\n",
    "    labels = np.concatenate([numeric_columns, encoded_category])\n",
    "    \n",
    "    preprocessed_df = pd.DataFrame(data=preprocessed_data, columns=labels)\n",
    "    return pd.concat([preprocessed_df, y], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_new = clean_data(df)\n",
    "processed_df= preprocess_data(df_new)\n",
    "processed_df.head()\n",
    "\n",
    "#df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile experiments/mlflow_eda.py\n",
    "\n",
    "from funct.myfuncs import cln_data\n",
    "import argparse\n",
    "\n",
    "experiment_name ='exp-Airlines'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    \n",
    "    #loading data \n",
    "    #dataset = Dataset.get_by_name(ws, name='AirlinesDelay')\n",
    "    df = pd.read_csv(\"../airlines_delay/airlines_delay.csv\", sep = \",\")\n",
    "    \n",
    "    #Distribution of the target column\n",
    "    class_cero, class_one = df['Class'].value_counts()\n",
    "    mlflow.log_metrics({'one':class_one, 'cero':class_cero})\n",
    "    \n",
    "    #Comparation of the mean\n",
    "    mean_comparison = df[['Time','Length', 'Class']].groupby('Class').mean().to_dict()\n",
    " \n",
    "    mlflow.log_metrics({'Mean Time class cero':mean_comparison['Time'][0], 'Mean Time class one':mean_comparison['Time'][1]})\n",
    "    mlflow.log_metrics({'Mean Lenght class cero':mean_comparison['Length'][0], 'Mean Length class one':mean_comparison['Length'][1]})\n",
    "\n",
    "    #Number of airlines\n",
    "    number_of_airlines = len(df['Airline'].unique())\n",
    "\n",
    "    mlflow.log_metric('number of airlines', number_of_airlines)\n",
    "\n",
    "    #flights per airlnes\n",
    "    flights_per_airline = df['Airline'].value_counts().to_dict()\n",
    "    mlflow.log_metrics(flights_per_airline)\n",
    "\n",
    "    \n",
    "    #airports\n",
    "    number_of_airports = len(df['AirportFrom'].unique())\n",
    "    mlflow.log_metric('number of airports', number_of_airports)\n",
    "\n",
    "    #airports with more activity\n",
    "    arports_activity_df = df['AirportFrom'].value_counts().sort_values(ascending=False) + df['AirportTo'].value_counts().sort_values(ascending=False)\n",
    "    top_5_airpots_activity = arports_activity_df.sort_values(ascending=False).head(5).to_dict()\n",
    "\n",
    "    top_and_lowest_ariports_activity = {}\n",
    "    for k, _ in top_5_airpots_activity.items():\n",
    "        top_and_lowest_ariports_activity[f'Airport {k}'] = top_5_airpots_activity[k]\n",
    "\n",
    "\n",
    "    #airports with more activity\n",
    "    lowest_5_airpots_activity = arports_activity_df.sort_values(ascending=False).tail(5).to_dict()\n",
    "    for k, _ in lowest_5_airpots_activity.items():\n",
    "        top_and_lowest_ariports_activity[f'Airport {k}'] = lowest_5_airpots_activity[k]\n",
    "\n",
    "    mlflow.log_metrics(top_and_lowest_ariports_activity)\n",
    "\n",
    "    #DaysWeek\n",
    "    dict_day_of_week= {1:'Monday', 2:'Tuesday', 3:'Wednesday', 4:'Thursday', 5:'Friday', 6:'Saturday', 7: 'Sunday'}\n",
    "    df_day_of_week = df.replace({\"DayOfWeek\": dict_day_of_week},inplace=False)\n",
    "    fligths_per_day_of_week = df_day_of_week['DayOfWeek'].value_counts().to_dict()\n",
    "\n",
    "    mlflow.log_metrics(fligths_per_day_of_week)\n",
    "\n",
    "\n",
    "    #plot distribution of time\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.gca()    \n",
    "    plt.hist(df['Length'])\n",
    "    plt.show()\n",
    "    mlflow.log_figure(fig, \"plot.png\")\n",
    "    print(\"Plot listo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train_data.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "x = df.drop('Class', axis=1, inplace=False)\n",
    "y = df['Class']\n",
    "\n",
    "'''\n",
    "lr_cv = cross_validate(LogisticRegression(max_iter=400), x, y, cv=2, scoring=['balanced_accuracy', 'recall', 'precision'], return_train_score=True)\n",
    "print('Listo regresion lineal')\n",
    "#rf_cv = cross_validate(RandomForestClassifier(), x, y, cv=3, scoring=['balanced_accuracy', 'recall', 'precision'], return_train_score=True)\n",
    "#print('Listo random forest')\n",
    "svm = cross_validate(SVC(), x, y, cv = 3, scoring=['balanced_accuracy', 'recall', 'precision'], return_train_score=True)\n",
    "print('Listo SVM')\n",
    "\n",
    "hola =[]\n",
    "for score_model, name_model in [(lr_cv, 'lr'), (svm, 'svm')]:\n",
    "    hola.append({\n",
    "        f'train_acc_{name_model}': np.mean(score_model['train_balanced_accuracy']),\n",
    "        f'train_recall_{name_model}': np.mean(score_model['train_recall']),\n",
    "        f'train_precision_{name_model}': np.mean(score_model['train_precision']),\n",
    "        f'test_acc_{name_model}': np.mean(score_model['test_balanced_accuracy']),\n",
    "        f'test_recall_{name_model}': np.mean(score_model['test_recall']),\n",
    "        f'test_precision_{name_model}': np.mean(score_model['test_precision'])\n",
    "    })\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "param_dist ={\n",
    "    'penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'C': [c/10 for c in range(1,20,5)],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "\n",
    "}\n",
    "clf = RandomizedSearchCV(LogisticRegression(), param_dist, n_iter=5, random_state=0, refit='balanced_accuracy')\n",
    "'''\n",
    "mlflow.log_params({\n",
    "    'penalty': 'l2',\n",
    "    'C': 1.0,\n",
    "    'solver':'lbfgs'\n",
    "})\n",
    "'''\n",
    "clf.fit(x,y)\n",
    "\n",
    "#mlflow.sklearn.log_model(lr, artifact_path='skleran-model')\n",
    "y_pred = clf.predict(x)\n",
    "\n",
    "recall = recall_score(y,y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(acc)\n",
    "print(conf_matrix)\n",
    "\n",
    "#logging.info(f'matriz de confusion{conf_matrix}')\n",
    "\n",
    "'''\n",
    "mlflow.log_metrics({\n",
    "    'recall':recall,\n",
    "    'precision': precision,\n",
    "    'accuracy': acc,\n",
    "\n",
    "})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "x = df.drop('Class', axis=1, inplace=False)\n",
    "y = df['Class']\n",
    "\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "svm.fit(x,y)\n",
    "\n",
    "y_pred = svm.predict(x)\n",
    "\n",
    "recall = recall_score(y,y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(acc)\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
