{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml\n",
    "#Importing libraries\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Experiment\n",
    "#from azureml.widgets import RunDetails\n",
    "from azureml.core import Run\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "import mlflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subir los datos a Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "airlines_delay = '../airlines_delay/airlines_delay.csv'\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([airlines_delay], \n",
    "                           target_path = 'airlines', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload calls completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer el dataset y dividirlo en 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore\n",
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "dataset = Dataset.get_by_name(ws, name='AirlinesDelay')\n",
    "airlines_df = dataset.to_pandas_dataframe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating...........................................\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, ComputeInstance\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# nombre del cluster\n",
    "compute_name = \"prueba-DS\"\n",
    "ws = Workspace.from_config()\n",
    "# verificaci√≥n de exixtencia del cluster\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print('Existe!')\n",
    "except ComputeTargetException:\n",
    "    \n",
    "    compute_config = ComputeInstance.provisioning_configuration(vm_size='Standard_DS11_v2',\n",
    "                                                           ssh_public_access=False)\n",
    "    aml_compute = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir el ambiente de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification(\"experiment_env\",  './env/environment.yml')\n",
    "\n",
    "# Register the environment \n",
    "experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'experiment_env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = aml_compute\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the source_directory for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the source_directory of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesamiento\n",
    "#1. eliminar columna que no importa\n",
    "#2. cambiar variables categoricas en numericas\n",
    "#3. Estandarizar variables numericas\n",
    "#4. split resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "import mlflow\n",
    "\n",
    "\n",
    "#Linkg workspace with mlflow\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "airlines_data = ws.datasets.get('AirlinesDelay')\n",
    "\n",
    "#https://learn.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py\n",
    "#clean_airlines_data = PipelineData(\"clean_airlines_data\", datastore=default_store, is_directory=True).as_dataset()\n",
    "clean_airlines_data = OutputFileDatasetConfig('cleaned_data')\n",
    "\n",
    "clean_step = PythonScriptStep(\n",
    "    name=\"Clean airlines data\",\n",
    "    script_name=\"scripts/data_clean.py\", \n",
    "    arguments=[\"--output_cleanse\", clean_airlines_data],\n",
    "    inputs=[airlines_data.as_named_input('raw_data')],\n",
    "    outputs=[clean_airlines_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "# train and test splits output\n",
    "output_split_train = OutputFileDatasetConfig(\"output_split_train\")\n",
    "output_split_test = OutputFileDatasetConfig(\"output_split_test\")\n",
    "output_split_validation = OutputFileDatasetConfig(\"output_split_validation\")\n",
    "\n",
    "train_test_split_step = PythonScriptStep(\n",
    "    name=\"split data train test\",\n",
    "    script_name=\"scripts/train_test_split.py\", \n",
    "    arguments=[\"--input_data\", clean_airlines_data.as_input(name='Clean_data'),\n",
    "            \"--output_train_data\", output_split_train,\n",
    "            \"--output_test_data\", output_split_test,\n",
    "            \"--output_val_data\", output_split_validation],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "transformed_data_train = OutputFileDatasetConfig('transformed_data_train')\n",
    "transformed_data_val = OutputFileDatasetConfig('transformed_data_val')\n",
    "scaler = PipelineData(\"scaler\", datastore=datastore)\n",
    "\n",
    "transform_step = PythonScriptStep(\n",
    "    name=\"transform airlines data\",\n",
    "    script_name=\"scripts/data_transform.py\", \n",
    "    arguments=['--input_data_train', output_split_train.as_input(name='Train_data'),\n",
    "            '--input_data_val',output_split_validation.as_input(name= 'Val_data'),\n",
    "            '--input_data_test', None,\n",
    "            \"--output_transform_train\", transformed_data_train,\n",
    "            \"--output_transform_val\", transformed_data_val,\n",
    "            \"--output_transform_test\", None,\n",
    "            \"--output_scaler\", scaler],\n",
    "    outputs=[scaler],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "step_output = PipelineData(\"model\", datastore=datastore)\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "    name = 'Training model',\n",
    "    script_name = 'scripts/train_model.py',\n",
    "    arguments = [\"--input_data_train\", transformed_data_train.as_input(name='train_Data'),\n",
    "                \"--input_data_val\", transformed_data_val.as_input(name=\"val_data\"),\n",
    "                \"--output_model\", step_output],\n",
    "    #inputs = [output_split_train],\n",
    "    outputs = [step_output],\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    "\n",
    ")\n",
    "\n",
    "validation_step = PythonScriptStep(\n",
    "    name = 'Validation model',\n",
    "    script_name = 'scripts/val_model.py',\n",
    "    arguments = [\"--model_out\", step_output.as_input(input_name='model_output'),\n",
    "                \"--input_scaler\", scaler.as_input(input_name='scaler')],\n",
    "    inputs = [step_output, scaler],\n",
    "    #outputs = [],\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [clean_step, train_test_split_step, transform_step, train_step, validation_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'exp-Airlines')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "#RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core import Dataset, Datastore\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core import Workspace\n",
    "import mlflow\n",
    "#Linkg workspace with mlflow\n",
    "ws = Workspace.from_config()\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "dataset_test = Dataset.get_by_name(ws, name='data_to_predict')\n",
    "processed_data = OutputFileDatasetConfig('proccesed_data')\n",
    "\n",
    "tracking_step = PythonScriptStep(\n",
    "    name = 'traking',\n",
    "    script_name=\"scripts_inference/tracking.py\", \n",
    "    #arguments=[\"--output_data\",processed_data],\n",
    "    inputs=[dataset_test.as_named_input('raw_data')],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    "    )\n",
    "\n",
    "preprocess_step = PythonScriptStep(\n",
    "    name = 'preprocessing',\n",
    "    script_name=\"scripts_inference/preprocess_data.py\", \n",
    "    arguments=[\"--output_data\",processed_data],\n",
    "    inputs=[dataset_test.as_named_input('raw_data')],\n",
    "    outputs=[processed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "inference_step = PythonScriptStep(\n",
    "    name = 'inference',\n",
    "    script_name = 'scripts_inference/inference.py',\n",
    "    arguments = ['--input_data', processed_data.as_input(name='processed_data')],\n",
    "    #inputs = [processed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps_inference = [tracking_step, preprocess_step ,inference_step]\n",
    "pipeline_inference = Pipeline(workspace=ws, steps=pipeline_steps_inference)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'exp-Airlines_inference')\n",
    "pipeline_run = experiment.submit(pipeline_inference, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "#RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment, Model, AmlCompute, Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: .\\config.json\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml_client.models.get(name='airlines_model', label='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BatchEndpoint(\n",
    "    name='my-first-endpoint',\n",
    "    description='my very first endpoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.ai.ml._restclient.v2022_05_01.models._models_py3.BatchEndpointData at 0x26ad7cd93a0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import BatchDeploymentOutputAction\n",
    "from azure.ai.ml.entities import BatchRetrySettings\n",
    "\n",
    "deployment = BatchDeployment(\n",
    "    name=\"first-deployment\",\n",
    "    description=\"airline classifier\",\n",
    "    endpoint_name=endpoint.name,\n",
    "    model=model,\n",
    "    environment=experiment_env,\n",
    "    compute=compute_name,\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=5,\n",
    "    output_action=BatchDeploymentOutputAction.APPEND_ROW,\n",
    "    output_file_name=\"predictions.csv\",\n",
    "    retry_settings=BatchRetrySettings(max_retries=1, timeout=300),\n",
    "    logging_level=\"info\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchDeployment({'deployment_type': 'Model', 'job_definition': None, 'endpoint_name': 'my-first-endpoint', 'type': None, 'name': 'first-deployment', 'description': 'airline classifier', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/20d4fdf3-6a4b-4f0b-a842-bd7392136332/resourceGroups/cienciadatos/providers/Microsoft.MachineLearningServices/workspaces/azureml/batchEndpoints/my-first-endpoint/deployments/first-deployment', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\pablo.tamayo\\\\Desktop\\\\DataSchool\\\\Azure\\\\AzureDataSchool', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000026AD7CF1D60>, 'model': '/subscriptions/20d4fdf3-6a4b-4f0b-a842-bd7392136332/resourceGroups/cienciadatos/providers/Microsoft.MachineLearningServices/workspaces/azureml/models/airlines_model/versions/4', 'code_configuration': None, 'environment': None, 'environment_variables': {}, 'compute': '/subscriptions/20d4fdf3-6a4b-4f0b-a842-bd7392136332/resourceGroups/cienciadatos/providers/Microsoft.MachineLearningServices/workspaces/azureml/computes/prueba-DS', 'resources': {'instance_count': 2, 'properties': {}}, 'output_action': 'append_row', 'output_file_name': 'predictions.csv', 'error_threshold': -1, 'retry_settings': <azure.ai.ml.entities._deployment.deployment_settings.BatchRetrySettings object at 0x0000026AD7B91F40>, 'logging_level': 'Info', 'mini_batch_size': 5, 'max_concurrency_per_instance': 2})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_predict = ml_client.data.get(name='data_to_predict', label='latest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
