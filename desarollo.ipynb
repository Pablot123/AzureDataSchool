{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml\n",
    "#Importing libraries\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Experiment\n",
    "#from azureml.widgets import RunDetails\n",
    "from azureml.core import Run\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "import mlflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subir los datos a Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "airlines_delay = '../airlines_delay/airlines_delay.csv'\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([airlines_delay], \n",
    "                           target_path = 'airlines', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload calls completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer el dataset y dividirlo en 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore\n",
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "dataset = Dataset.get_by_name(ws, name='AirlinesDelay')\n",
    "airlines_df = dataset.to_pandas_dataframe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating...........................................\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, ComputeInstance\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# nombre del cluster\n",
    "compute_name = \"prueba-DS\"\n",
    "ws = Workspace.from_config()\n",
    "# verificaci√≥n de exixtencia del cluster\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print('Existe!')\n",
    "except ComputeTargetException:\n",
    "    \n",
    "    compute_config = ComputeInstance.provisioning_configuration(vm_size='Standard_DS11_v2',\n",
    "                                                           ssh_public_access=False)\n",
    "    aml_compute = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir el ambiente de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification(\"experiment_env\",  './env/environment.yml')\n",
    "\n",
    "# Register the environment \n",
    "experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'experiment_env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = aml_compute\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the source_directory for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the source_directory of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesamiento\n",
    "#1. eliminar columna que no importa\n",
    "#2. cambiar variables categoricas en numericas\n",
    "#3. Estandarizar variables numericas\n",
    "#4. split resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "import mlflow\n",
    "\n",
    "\n",
    "#Linkg workspace with mlflow\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "airlines_data = ws.datasets.get('AirlinesDelay')\n",
    "\n",
    "#https://learn.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py\n",
    "#clean_airlines_data = PipelineData(\"clean_airlines_data\", datastore=default_store, is_directory=True).as_dataset()\n",
    "clean_airlines_data = OutputFileDatasetConfig('cleaned_data')\n",
    "\n",
    "clean_step = PythonScriptStep(\n",
    "    name=\"Clean airlines data\",\n",
    "    script_name=\"scripts/data_clean.py\", \n",
    "    arguments=[\"--output_cleanse\", clean_airlines_data],\n",
    "    inputs=[airlines_data.as_named_input('raw_data')],\n",
    "    outputs=[clean_airlines_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "# train and test splits output\n",
    "output_split_train = OutputFileDatasetConfig(\"output_split_train\")\n",
    "output_split_test = OutputFileDatasetConfig(\"output_split_test\")\n",
    "output_split_validation = OutputFileDatasetConfig(\"output_split_validation\")\n",
    "\n",
    "train_test_split_step = PythonScriptStep(\n",
    "    name=\"split data train test\",\n",
    "    script_name=\"scripts/train_test_split.py\", \n",
    "    arguments=[\"--input_data\", clean_airlines_data.as_input(name='Clean_data'),\n",
    "            \"--output_train_data\", output_split_train,\n",
    "            \"--output_test_data\", output_split_test,\n",
    "            \"--output_val_data\", output_split_validation],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "transformed_data_train = OutputFileDatasetConfig('transformed_data_train')\n",
    "transformed_data_val = OutputFileDatasetConfig('transformed_data_val')\n",
    "scaler = PipelineData(\"scaler\", datastore=datastore)\n",
    "\n",
    "transform_step = PythonScriptStep(\n",
    "    name=\"transform airlines data\",\n",
    "    script_name=\"scripts/data_transform.py\", \n",
    "    arguments=['--input_data_train', output_split_train.as_input(name='Train_data'),\n",
    "            '--input_data_val',output_split_validation.as_input(name= 'Val_data'),\n",
    "            '--input_data_test', None,\n",
    "            \"--output_transform_train\", transformed_data_train,\n",
    "            \"--output_transform_val\", transformed_data_val,\n",
    "            \"--output_transform_test\", None,\n",
    "            \"--output_scaler\", scaler],\n",
    "    outputs=[scaler],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "step_output = PipelineData(\"model\", datastore=datastore)\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "    name = 'Training model',\n",
    "    script_name = 'scripts/train_model.py',\n",
    "    arguments = [\"--input_data_train\", transformed_data_train.as_input(name='train_Data'),\n",
    "                \"--input_data_val\", transformed_data_val.as_input(name=\"val_data\"),\n",
    "                \"--output_model\", step_output],\n",
    "    #inputs = [output_split_train],\n",
    "    outputs = [step_output],\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    "\n",
    ")\n",
    "\n",
    "validation_step = PythonScriptStep(\n",
    "    name = 'Validation model',\n",
    "    script_name = 'scripts/val_model.py',\n",
    "    arguments = [\"--model_out\", step_output.as_input(input_name='model_output'),\n",
    "                \"--input_scaler\", scaler.as_input(input_name='scaler')],\n",
    "    inputs = [step_output, scaler],\n",
    "    #outputs = [],\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [clean_step, train_test_split_step, transform_step, train_step, validation_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'exp-Airlines')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "#RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core import Dataset, Datastore\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core import Workspace\n",
    "import mlflow\n",
    "#Linkg workspace with mlflow\n",
    "ws = Workspace.from_config()\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "dataset_test = Dataset.get_by_name(ws, name='data_to_predict')\n",
    "processed_data = OutputFileDatasetConfig('proccesed_data')\n",
    "\n",
    "tracking_step = PythonScriptStep(\n",
    "    name = 'traking',\n",
    "    script_name=\"scripts_inference/tracking.py\", \n",
    "    #arguments=[\"--output_data\",processed_data],\n",
    "    inputs=[dataset_test.as_named_input('raw_data')],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    "    )\n",
    "\n",
    "preprocess_step = PythonScriptStep(\n",
    "    name = 'preprocessing',\n",
    "    script_name=\"scripts_inference/preprocess_data.py\", \n",
    "    arguments=[\"--output_data\",processed_data],\n",
    "    inputs=[dataset_test.as_named_input('raw_data')],\n",
    "    outputs=[processed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "inference_step = PythonScriptStep(\n",
    "    name = 'inference',\n",
    "    script_name = 'scripts_inference/inference.py',\n",
    "    arguments = ['--input_data', processed_data.as_input(name='processed_data')],\n",
    "    #inputs = [processed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=pipeline_run_config,\n",
    "    source_directory='./',\n",
    "    allow_reuse=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps_inference = [tracking_step, preprocess_step ,inference_step]\n",
    "pipeline_inference = Pipeline(workspace=ws, steps=pipeline_steps_inference)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'exp-Airlines_inference')\n",
    "pipeline_run = experiment.submit(pipeline_inference, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "#RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps_deployment = [preprocess_step ,inference_step]\n",
    "pipeline_deployment = Pipeline(workspace=ws, steps=pipeline_steps_deployment)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'deployment_pipeline')\n",
    "deployenent_pipeline_run = experiment.submit(pipeline_deployment, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "#RunDetails(pipeline_run).show()\n",
    "deployenent_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "published_pipeline = deployenent_pipeline_run.publish_pipeline(name=\"My_Published_Pipeline\", \n",
    "description=\"My Published Pipeline Description\",\n",
    "version=\"1.0\")\n",
    "#endpoint = PipelineEndpoint.publish(workspace=ws, name=\"my-pipeline-endpoint\", description='my very first pipeline', pipeline=published_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication header ready.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "print('Authentication header ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "import requests\n",
    "\n",
    "response = requests.post(published_pipeline.endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"deployment_pipeline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ae78320d-c53a-474f-89d1-95f96faf04f6'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = response.json()[\"Id\"]\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments['deployment_pipeline'], run_id)\n",
    "\n",
    "# Block until the run completes\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "# Define the recurrence\n",
    "recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=10)\n",
    "\n",
    "# Define the schedule\n",
    "experiment_name = \"schedule-deploy\"\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "schedule = Schedule.create(workspace=ws,experiment_name= experiment_name, name=\"my-schedule\", pipeline_id='105374ac-b0da-4810-b295-d841d7f1e27e', description=\"Runs my pipeline daily\", recurrence=recurrence)\n",
    "\n",
    "# Trigger the pipeline on the scheduled date and time\n",
    "#schedule.submit(experiment=experiment)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.disable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
